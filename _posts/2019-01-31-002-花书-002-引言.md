---
layout:     post
title:      002-花书-002-引言
subtitle:    "\"002-花书-002-引言\""
date:       2019-01-31
author:     PZ
header-img: img/post-bg-2015.jpg
catalog: true
tags:
    - DL
    - 读书笔记
    - 花书
---

## 002-花书-002-引言

[TOC]

### 1 引言

![image.png](https://upload-images.jianshu.io/upload_images/14744153-8c363966042e76ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### 1 一切的一切... 表征

##### 1 `知识库`式的hardcode

> 在AI的历程中，存在在两个学派`符号主义`和`联接主义`学派，符号主义学派，在早期的人工智能研究阶段，希望通过对人类思维方式的符号表示来，完成智能化，一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码 (**hardcode**)，知识库的概念一直存在着，但是这显然实现起来十分困难，`hardcode`如何能编写人类的思维呢，人的一个念想，可能转化为code那就是成百上千的ifelse了。 **有多少智能，就有多少可怜的码农人工的coding**

> 无法从人类的知识逻辑中，进行总结，另一个思路将重点放到了数据上，like：逻辑回归（logistic regression）来建议是否剖腹产，朴素贝叶
斯（naive Bayes）算法完成，垃圾电子邮件的分类。

- 表征的问题

> 无论是判断剖腹产和垃圾邮件分类，要面临的第一个问题都是数据的表征，对于剖腹产你拥有的数据可能是，各种检查设备的图片、化验单已经医生长期跟踪的病例信息。这些信息是无法直接输入到模型的，信息是要被结构化和经验筛选的，如何根据B超检查`是否存在子宫疤痕`，`孕妇的体重`等等，这些特征的提取依赖与对专业知识的掌握，（这里和专家知识库似乎又有了一些联系）但对于机器学习来说，这是一个粗粒度的。whatever，输入信息的表征，这仍然是一个如识别库建立一样耗时耗人的工作。

> 根悲观的是，对于许多任务来说，我们很难知道应该提取哪些特征。比如在人类感知层的智能（图像识别，语音识别等），我们对于自己是如何获得感知能力的机理都研究不深的情况下，很难知道该选什么特征了。

- 表征学习（representation learning）

> 鉴于这些简单模型只能从专家提供的表征映射到结果，而不能自己提取出特征的局限性，人们发展出了表征学习(representation learning)，希望机器自己能够提取出有意义的特征而无需人为干预。经典的例子就是自编码器Autoencoder，主要就是由编码器encoder从原始数据提取特征，然后可以通过解码器decoder利用新的表征来重塑原始数据。

tip: 表征中`的变差因素`影响



#### 2 分层表征

![image.png](https://upload-images.jianshu.io/upload_images/14744153-24c347a3fae8971b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 这些表征很有可能是隐含的、抽象的，比如图像识别中单个像素可能没有有效的信息，更有意义的是若干像素组成的边，由边组成的轮廓，进而由轮廓组成的物体。深度学习就是通过一层层的表征学习，每层可能逻辑很简单，但之后的层可以通过对前面简单的层的组合来构建更复杂的表征。经典的例子如多层感知机(multilayer perceptron),就是每个感知机的数理逻辑都很简单，层内可以并行执行，层间顺序执行，通过层层叠加实现更复杂的逻辑。深度学习的“深”可以理解为通过更多层来结合出更复杂的逻辑，这就完成了从输入到内在层层表征再由内在表征到输出的映射。


![image.png](https://upload-images.jianshu.io/upload_images/14744153-5301ff8a6824954b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 总结： 深度学习就是中通过输入基础特征，然后通过模型进行分层表征的表征学习


#### 3 Deep Learning ，你的名字

> 深度学习这个词火起来是从2012年 ImageNet比赛开始的，但是深度学习的思想却由来已久，在不同的历史时期，有着不同的名字。而这可以从人工智能的三次浪潮讲起：

- 1940-1960 AI第一次兴起：

> 随着现在电子计算机在二战的时期的成果制造和应用，对于AI便顺势兴起。人们从神经科学找灵感，希望能够搭建能够模拟人脑工作模式的神经网络，这个时期的名字是人工神经网络（Artificial Neural Network) 或神经机械学（cybernetics)。感知机模型（perceptron)也是这个时期的产物。这个时期的模型大部分都是线性模型，对于非线性的关系（异或问题）不能进行很好的模拟所以有很大的局限性，深度学习研究就逐渐降温了。不过，这个时期为后来的深度学习打下基础，我们现在训练常用的随机梯度下降算法（stochastic gradient descent)就是源自处理这个时期的一种线性模型——自适应线性单元（Adaptive Linear Neuron）。虽然现在仍有媒体经常将深度学习与神经科学类比，但是由于我们对大脑工作机制的研究进展缓慢，所以实际上现在深度学习从业者已很少从神经科学中寻找灵感。

- 1980-1990 AI第二次兴起

> 这个时候更多的称为联结主义（connectionism)或并行分布式计算（parallel distributed computing)，主要是强调很多的简单的计算单位可以通过互联进行更复杂的计算。这个时期成果很多，比如现在常用的反向传播算法（back propagation)还有自然语言处理中常用的LSTM(第十章讲递归神经网络时详谈）都来自与这个时期。之后由于很多AI产品期望过高而又无法落地，研究热潮逐渐退去。但其间产生的概念如**分布式表示** **反向传播**在现在的深度学习中都是极为重要的概念。

- 2006-Now

> 第三个阶段就是2006年至今，由于软硬件性能的提高，深度学习逐渐应用在各个领域，深度学习研究重整旗鼓。“深度”学习的名字成为主流，意在强调可以训练更多层次更复杂的神经网络，“深”帮助我们开发更复杂的模型，解决更复杂的问题。

```
。Geoffrey Hinton 表明名为深
度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练
(Hinton et al., 2006a)
```

#### 4 总结

- 1 深度学习历史悠久，在不同时期名字不同，代表不同的侧重点。
- 2 由于训练数据的增多和软硬件性能的提高，深度学习的模型越来越准确。
- 3 深度学习逐渐走向实用化。


